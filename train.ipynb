{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/csv\"\n",
    "root = \"/Users/qiaowenyang/Desktop/newpj/chatbot\"\n",
    "files = os.listdir(path)\n",
    "pairs = []\n",
    "for file in files:\n",
    "    fullpath = os.path.join(path, file)\n",
    "    f = open(fullpath, 'r')\n",
    "    reader = csv.reader(_.replace('\\x00', '') for _ in f)\n",
    "    lst = list(reader)\n",
    "    for i in range(len(lst)):\n",
    "        if len(lst[i]) < 4:\n",
    "            continue\n",
    "        content = lst[i][3]\n",
    "        if len(content) > 11 and content[6:11] == \"emoji\":\n",
    "            idx = content.find(\"thumburl\") # 只提取静态表情\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            left = content.find(\"\\\"\", idx)\n",
    "            right = content.find(\"\\\"\", left + 1)\n",
    "            url = content[left + 1:right]\n",
    "            if url == \"\":\n",
    "                continue\n",
    "            if i > 1 and len(lst[i - 1]) > 6 and lst[i - 1][6] == '1': # 表情的上一句需是文字\n",
    "                last_text = lst[i - 1][3]\n",
    "                if last_text.find(\"wxid_\") != -1: # 处理群聊中的wxid\n",
    "                    colon_idx = last_text.find(\":\")\n",
    "                    last_text = last_text[colon_idx + 2:]\n",
    "                pair = []\n",
    "                pair.append(last_text)\n",
    "                pair.append(url)\n",
    "                pairs.append(pair)\n",
    "    f.close()\n",
    "#print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pairs_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/pairs.csv\"\\nwith open(pairs_path, \\'w\\', newline=\\'\\') as f:\\n    writer = csv.writer(f)\\n    for row in pairs:\\n        writer.writerow(row)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pairs_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/pairs.csv\"\n",
    "with open(pairs_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in pairs:\n",
    "        writer.writerow(row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3393\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/train.csv\"\n",
    "test_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/test.csv\"\n",
    "shuffle_pairs = pairs\n",
    "random.shuffle(shuffle_pairs)\n",
    "train_num = int(len(pairs) * 0.8)\n",
    "trainset = shuffle_pairs[:train_num]\n",
    "testset = shuffle_pairs[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open(train_path, 'w', newline='') as f:\\n    writer = csv.writer(f)\\n    for row in trainset:\\n        writer.writerow(row)\\nwith open(test_path, 'w', newline='') as f:\\n    writer = csv.writer(f)\\n    for row in testset:\\n        writer.writerow(row)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open(train_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in trainset:\n",
    "        writer.writerow(row)\n",
    "with open(test_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in testset:\n",
    "        writer.writerow(row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for pair in pairs:\n",
    "    dict[pair[1]] = dict.get(pair[1], 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JM4VOicLMB1H8ldGoG6pf6YSWq0ic3dP0hsI1yibXwJoHyF/0', 339), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLALxSLBT9ia2YKwibicvnJT1lcGSPseOJib9Iibic4PK3OgQhNiaPPDznicicTV0/0', 116), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAqn9uGg0riawTIoqtibyenzzoeRRXpQT1ME9Og5jIEicEI39WMYbX1LLM/0', 96), ('http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM5llZ8F17rYSxaT0rHd1ibtq83VqoK8HNiadrBn4c79H7IDZqxCzKTD98/0', 64), ('http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM7lf1vRwFDtwHoCmelDVIGiaLB6x8wMtR0yiaNQ7lK4LUXfsf2hIpex9D/0', 63), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLBVWrxFJdiaibzrsehMjZZU1xtm9xTUacErlzRLK7jbibuo0pSRES1IoCo/0', 47), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCSGzU8cqeRmFMDDiaw2KKbpJfic0AqHnkugYtKRbdUznnF9uUjcW7sM9/0', 47), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JNk0zs9dNFASHTt5adwwV6ZcaO3ImnSwjnDj5LgBvjxy/0', 47), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAhXBBjIqSl2TVadDMGVSJN7H9Bty3NjyiaE7ibCuibHcLz07kMRWT0g7v/0', 44), ('http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLC3ib2qk5SSCGLN1GGzwwphbh5ec1NXqLPsB0lZ6woHJgxvgOsLKJNvg/0', 42)]\n"
     ]
    }
   ],
   "source": [
    "sorted_dict = sorted(dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_dict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905\n"
     ]
    }
   ],
   "source": [
    "top_10_dict = sorted_dict[:10]\n",
    "small_pairs = []\n",
    "for (k, v) in top_10_dict:\n",
    "    for pair in pairs:\n",
    "        if pair[1] == k:\n",
    "            small_pairs.append(pair)\n",
    "print(len(small_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "small_train_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/smalltrain.csv\"\n",
    "small_test_path = \"/Users/qiaowenyang/Desktop/newpj/chatbot/pairs/smalltest.csv\"\n",
    "small_shuffle_pairs = small_pairs\n",
    "random.shuffle(small_shuffle_pairs)\n",
    "small_train_num = int(len(small_pairs) * 0.8)\n",
    "small_trainset = small_shuffle_pairs[:small_train_num]\n",
    "small_testset = small_shuffle_pairs[small_train_num:]\n",
    "print(len(small_trainset))\n",
    "print(len(small_testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"with open(small_train_path, 'w', newline='') as f:\\n    writer = csv.writer(f)\\n    for row in small_trainset:\\n        writer.writerow(row)\\nwith open(small_test_path, 'w', newline='') as f:\\n    writer = csv.writer(f)\\n    for row in small_testset:\\n        writer.writerow(row)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open(small_train_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in small_trainset:\n",
    "        writer.writerow(row)\n",
    "with open(small_test_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in small_testset:\n",
    "        writer.writerow(row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kv/sdwj_qkj3yq7221vf10mzbh80000gn/T/jieba.cache\n",
      "Loading model cost 0.677 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_pairs = []\n",
    "for pair in small_trainset:\n",
    "    sent = pair[0]\n",
    "    meme = pair[1]\n",
    "    seg_list = jieba.cut(sent)\n",
    "    new_pair = [list(seg_list), meme]\n",
    "    seg_pairs.append(new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load('1000000.bin')\n",
    "w_size = model.wv.syn0[0].shape[0]\n",
    "vec_pairs = []\n",
    "sent_idx = []\n",
    "for pair in seg_pairs:\n",
    "    seg_list = pair[0]\n",
    "    meme = pair[1]\n",
    "    sent_vec = [0] * w_size\n",
    "    count = 0\n",
    "    for w in seg_list:\n",
    "        if w in model.wv.vocab.keys():\n",
    "            sent_vec += model[w]\n",
    "            count = count + 1\n",
    "    if count != 0:\n",
    "        for item in sent_vec:\n",
    "            item /= count\n",
    "        new_pair = [sent_vec, meme]\n",
    "        vec_pairs.append(new_pair)\n",
    "        sent_idx.append([sent_vec, ''.join(seg_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(w_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seg_pairs = []\n",
    "for pair in small_testset:\n",
    "    sent = pair[0]\n",
    "    meme = pair[1]\n",
    "    seg_list = jieba.cut(sent)\n",
    "    new_pair = [list(seg_list), meme]\n",
    "    test_seg_pairs.append(new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load('1000000.bin')\n",
    "w_size = model.wv.syn0[0].shape[0]\n",
    "test_vec_pairs = []\n",
    "test_sent_idx = []\n",
    "for pair in test_seg_pairs:\n",
    "    seg_list = pair[0]\n",
    "    meme = pair[1]\n",
    "    sent_vec = [0] * w_size\n",
    "    count = 0\n",
    "    for w in seg_list:\n",
    "        if w in model.wv.vocab.keys():\n",
    "            sent_vec += model[w]\n",
    "            count = count + 1\n",
    "    if count != 0:\n",
    "        for item in sent_vec:\n",
    "            item /= count\n",
    "        new_pair = [sent_vec, meme]\n",
    "        test_vec_pairs.append(new_pair)\n",
    "        test_sent_idx.append([sent_vec, ''.join(seg_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JM4VOicLMB1H8ldGoG6pf6YSWq0ic3dP0hsI1yibXwJoHyF/0': 0, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLALxSLBT9ia2YKwibicvnJT1lcGSPseOJib9Iibic4PK3OgQhNiaPPDznicicTV0/0': 1, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAqn9uGg0riawTIoqtibyenzzoeRRXpQT1ME9Og5jIEicEI39WMYbX1LLM/0': 2, 'http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM5llZ8F17rYSxaT0rHd1ibtq83VqoK8HNiadrBn4c79H7IDZqxCzKTD98/0': 3, 'http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM7lf1vRwFDtwHoCmelDVIGiaLB6x8wMtR0yiaNQ7lK4LUXfsf2hIpex9D/0': 4, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLBVWrxFJdiaibzrsehMjZZU1xtm9xTUacErlzRLK7jbibuo0pSRES1IoCo/0': 5, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCSGzU8cqeRmFMDDiaw2KKbpJfic0AqHnkugYtKRbdUznnF9uUjcW7sM9/0': 6, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JNk0zs9dNFASHTt5adwwV6ZcaO3ImnSwjnDj5LgBvjxy/0': 7, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAhXBBjIqSl2TVadDMGVSJN7H9Bty3NjyiaE7ibCuibHcLz07kMRWT0g7v/0': 8, 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLC3ib2qk5SSCGLN1GGzwwphbh5ec1NXqLPsB0lZ6woHJgxvgOsLKJNvg/0': 9}\n",
      "{0: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JM4VOicLMB1H8ldGoG6pf6YSWq0ic3dP0hsI1yibXwJoHyF/0', 1: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLALxSLBT9ia2YKwibicvnJT1lcGSPseOJib9Iibic4PK3OgQhNiaPPDznicicTV0/0', 2: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAqn9uGg0riawTIoqtibyenzzoeRRXpQT1ME9Og5jIEicEI39WMYbX1LLM/0', 3: 'http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM5llZ8F17rYSxaT0rHd1ibtq83VqoK8HNiadrBn4c79H7IDZqxCzKTD98/0', 4: 'http://mmbiz.qpic.cn/mmemoticon/Q3auHgzwzM7lf1vRwFDtwHoCmelDVIGiaLB6x8wMtR0yiaNQ7lK4LUXfsf2hIpex9D/0', 5: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLBVWrxFJdiaibzrsehMjZZU1xtm9xTUacErlzRLK7jbibuo0pSRES1IoCo/0', 6: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCSGzU8cqeRmFMDDiaw2KKbpJfic0AqHnkugYtKRbdUznnF9uUjcW7sM9/0', 7: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLCdwOLyl7c6JNk0zs9dNFASHTt5adwwV6ZcaO3ImnSwjnDj5LgBvjxy/0', 8: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLAhXBBjIqSl2TVadDMGVSJN7H9Bty3NjyiaE7ibCuibHcLz07kMRWT0g7v/0', 9: 'http://mmbiz.qpic.cn/mmemoticon/ajNVdqHZLLC3ib2qk5SSCGLN1GGzwwphbh5ec1NXqLPsB0lZ6woHJgxvgOsLKJNvg/0'}\n"
     ]
    }
   ],
   "source": [
    "url_map_rev = {}\n",
    "url_map = {}\n",
    "i = 0\n",
    "for (k, v) in top_10_dict:\n",
    "    url_map_rev[k] = i\n",
    "    url_map[i] = k\n",
    "    i += 1\n",
    "print(url_map_rev)\n",
    "print(url_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i, pair in enumerate(vec_pairs):\n",
    "    sent = pair[0]\n",
    "    meme = pair[1]\n",
    "    idx = url_map_rev[meme]\n",
    "    vec_pairs[i] = [sent, idx]\n",
    "\n",
    "for i, pair in enumerate(test_vec_pairs):\n",
    "    sent = pair[0]\n",
    "    meme = pair[1]\n",
    "    idx = url_map_rev[meme]\n",
    "    test_vec_pairs[i] = [sent, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_txt = os.path.join(root, \"train_vec.txt\")\n",
    "test_vec_txt = os.path.join(root, \"test_vec.txt\")\n",
    "f = open(train_vec_txt, 'w', encoding='utf-8')\n",
    "for row in vec_pairs:\n",
    "    row = str(row)\n",
    "    f.write(row)\n",
    "f.close()\n",
    "f = open(test_vec_txt, 'w', encoding='utf-8')\n",
    "for row in test_vec_pairs:\n",
    "    row = str(row)\n",
    "    f.write(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_pairs：训练集，每个pair是句向量对应这个句子的表情包链接\n",
    "# 用MLP进行分类：10个表情包，输入句向量size是1 * 200，输出size是10\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# network build\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(200, 100) # embedding_dim, hidden_size\n",
    "        self.drop1 = torch.nn.Dropout(0.1)\n",
    "        self.fc2 = torch.nn.Linear(100, 50) # hidden_size, hidden_size\n",
    "        self.drop2 = torch.nn.Dropout(0.1)\n",
    "        self.fc3 = torch.nn.Linear(50, 10) # hidden_size, output_size\n",
    "        \n",
    "    def forward(self, din):\n",
    "        din = din.view(-1, 200) # din: batchsize * embedding_dim: 100 * 200\n",
    "        dout = torch.nn.functional.relu(self.drop1(self.fc1(din))) # batchsize * hidden_size\n",
    "        dout = torch.nn.functional.relu(self.drop2(self.fc2(dout))) # batchsize * hidden_size\n",
    "        return self.fc3(dout) # 输出长度为10，代表对每一个类别挑选的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training:\n",
      "epoch: 0, loss: 2.0966\n",
      "train accuracy: 0.3273\n",
      "val accuracy: 0.3778\n",
      "======================\n",
      "epoch: 10, loss: 1.7455\n",
      "train accuracy: 0.3703\n",
      "val accuracy: 0.3833\n",
      "======================\n",
      "epoch: 20, loss: 1.3530\n",
      "train accuracy: 0.3786\n",
      "val accuracy: 0.3444\n",
      "======================\n",
      "epoch: 30, loss: 1.2874\n",
      "train accuracy: 0.4133\n",
      "val accuracy: 0.3611\n",
      "======================\n",
      "epoch: 40, loss: 0.9952\n",
      "train accuracy: 0.4424\n",
      "val accuracy: 0.2944\n",
      "======================\n",
      "epoch: 50, loss: 1.0569\n",
      "train accuracy: 0.4202\n",
      "val accuracy: 0.3833\n",
      "======================\n",
      "epoch: 60, loss: 0.7624\n",
      "train accuracy: 0.5118\n",
      "val accuracy: 0.3000\n",
      "======================\n",
      "epoch: 70, loss: 1.0727\n",
      "train accuracy: 0.4272\n",
      "val accuracy: 0.3278\n",
      "======================\n",
      "epoch: 80, loss: 0.8454\n",
      "train accuracy: 0.4521\n",
      "val accuracy: 0.3333\n",
      "======================\n",
      "epoch: 90, loss: 0.9199\n",
      "train accuracy: 0.4591\n",
      "val accuracy: 0.3444\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "# loss\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-2, lr=1e-2)\n",
    "n_epoch = 100\n",
    "loss = 0.0\n",
    "batchsize = 100\n",
    "\n",
    "def validation():\n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    for i in range(int(len(test_vec_pairs) / batchsize) + 1): # test_vec_pairs: totalnum * 2\n",
    "        batch = []\n",
    "        if i == int(len(test_vec_pairs) / batchsize):\n",
    "            batch = test_vec_pairs[batchsize * i:]\n",
    "        else:\n",
    "            batch = test_vec_pairs[batchsize * i: batchsize * (i + 1)] # 一个batch\n",
    "        input = [batch[k][0] for k in range(len(batch))]\n",
    "        label = [batch[k][1] for k in range(len(batch))]\n",
    "        input = torch.Tensor(input)\n",
    "        label = torch.LongTensor(label)\n",
    "        out = model(input)\n",
    "        with torch.no_grad():\n",
    "            predicted_label = torch.argmax(out, dim=1)\n",
    "        count_total += len(input)\n",
    "        count_correct += (label == predicted_label).sum().item()\n",
    "    accuracy = float(count_correct) / count_total\n",
    "    print('val accuracy: %.4f' % accuracy)\n",
    "\n",
    "print(\"start training:\")\n",
    "for i in range(n_epoch):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for j in range(int(len(vec_pairs) / batchsize) + 1):\n",
    "        batch = []\n",
    "        if j == int(len(vec_pairs) / batchsize):\n",
    "            batch = vec_pairs[batchsize * j:]\n",
    "        else:\n",
    "            batch = vec_pairs[batchsize * j: batchsize * (j + 1)]\n",
    "        optimizer.zero_grad()\n",
    "        input = [batch[k][0] for k in range(len(batch))]\n",
    "        label = [batch[k][1] for k in range(len(batch))]\n",
    "        input = torch.Tensor(input)\n",
    "        label = torch.LongTensor(label)\n",
    "        out = model(input)\n",
    "        with torch.no_grad():\n",
    "            predicted_label = torch.argmax(out, dim=1)\n",
    "        count1 += len(input)\n",
    "        count2 += (label == predicted_label).sum().item()\n",
    "        loss = loss1(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print('batch: %d, loss: %.4f' % (j, loss.item()))\n",
    "        #validation()\n",
    "        #print(\"----------------------\")\n",
    "    #print('epoch: %d, loss: %.4f' % (i, loss.item()))\n",
    "    if count2 != 0:\n",
    "        accuracy = float(count2) / count1\n",
    "        if i % 10 == 0:\n",
    "            print('epoch: %d, loss: %.4f' % (i, loss.item()))\n",
    "            print('train accuracy: %.4f' % accuracy)\n",
    "    if i % 10 == 0:\n",
    "        validation()\n",
    "        print(\"======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['可以', '麻烦', '帮', '我', '烧', '一下', '水嘛'], ['nlp', '让', '我', '焦头烂额'], ['今天天气', '好好'], ['写', '一首', '诗', '给', '我', '看看'], ['能', '不能', '帮', '她', '写个', '代码']]\n"
     ]
    }
   ],
   "source": [
    "demo = [\"可以麻烦帮我烧一下水嘛\", \"nlp让我焦头烂额\", \"今天天气好好\", \"写一首诗给我看看\", \"能不能帮她写个代码\"]\n",
    "\n",
    "demo_seg = []\n",
    "for sent in demo:\n",
    "    seg_list = jieba.cut(sent)\n",
    "    demo_seg.append(list(seg_list))\n",
    "print(demo_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/Users/qiaowenyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "demo_model = KeyedVectors.load('1000000.bin')\n",
    "w_size = demo_model.wv.syn0[0].shape[0]\n",
    "demo_vec_pairs = []\n",
    "for seg in demo_seg:\n",
    "    sent_vec = [0] * w_size\n",
    "    count = 0\n",
    "    for w in seg:\n",
    "        if w in demo_model.wv.vocab.keys():\n",
    "            sent_vec += demo_model[w]\n",
    "            count = count + 1\n",
    "    if count != 0:\n",
    "        for item in sent_vec:\n",
    "            item /= count\n",
    "        demo_vec_pairs.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 1, 3, 1, 0], grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "demo_vec_pairs = torch.Tensor(demo_vec_pairs)\n",
    "demo_out = model(demo_vec_pairs)\n",
    "predicted_demo_label = torch.argmax(demo_out, dim=1)\n",
    "print(predicted_demo_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以麻烦帮我烧一下水嘛\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp让我焦头烂额\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气好好\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写一首诗给我看看\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "能不能帮她写个代码\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage import io\n",
    "for i, label in enumerate(predicted_demo_label):\n",
    "    print(demo[i])\n",
    "    demo_url = url_map[int(label)]\n",
    "    image = io.imread(demo_url)\n",
    "    io.imshow(image)\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondac398b823a387452389d3d9b405b8505b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
